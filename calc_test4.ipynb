{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "# conda install onnx onnxruntime\n",
    "# pip install torch torchvision numpy opencv-python seaborn matplotlib scikit-learn pillow onnxscript\n",
    "\n",
    "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# pip3 install onnx onnxruntime onnxscript ipykernel quanto tqdm\n",
    "# sudo apt-get install g++ ???????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import quanto\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from matching import matching\n",
    "from evaluation.metrics import createPR, recallAt100precision, recallAtK\n",
    "from datasets.load_dataset import GardensPointDataset, SFUDataset, StLuciaDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_FILE = \"calc.caffemodel.pt\"\n",
    "ITERATIONS = 100 # for testing average duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Load dataset SFU dry--jan\n"
     ]
    }
   ],
   "source": [
    "imgs_db, imgs_q, GThard, GTsoft = SFUDataset().load()\n",
    "# imgs_db, imgs_q, GThard, GTsoft = GardensPointDataset().load()\n",
    "# imgs_db, imgs_q, GThard, GTsoft = StLuciaDataset().load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToYUVandEqualizeHist:\n",
    "    def __call__(self, img):\n",
    "        img_yuv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2YUV)\n",
    "        img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :, 0])\n",
    "        img_rgb = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)\n",
    "        return Image.fromarray(img_rgb)\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        ConvertToYUVandEqualizeHist(),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((120, 160), interpolation=Image.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([385, 1, 120, 160])\n"
     ]
    }
   ],
   "source": [
    "# Preprocess db images\n",
    "\n",
    "db_tensor = []\n",
    "\n",
    "for image in imgs_db:\n",
    "    preprocessed_image = preprocess(image)\n",
    "    db_tensor.append(preprocessed_image)\n",
    "\n",
    "db_tensor = torch.stack(db_tensor, axis=0)\n",
    "\n",
    "# Convert numpy array to a tensor\n",
    "# query_tensor = torch.from_numpy(query_matrix)\n",
    "\n",
    "print(db_tensor.shape)\n",
    "# print(query_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([385, 1, 120, 160])\n"
     ]
    }
   ],
   "source": [
    "# Preprocess query images\n",
    "\n",
    "q_tensor = []\n",
    "\n",
    "for image in imgs_q:\n",
    "    preprocessed_image = preprocess(image)\n",
    "    q_tensor.append(preprocessed_image)\n",
    "\n",
    "q_tensor = torch.stack(q_tensor, axis=0)\n",
    "\n",
    "# Convert numpy array to a tensor\n",
    "# query_tensor = torch.from_numpy(map_matrix)\n",
    "\n",
    "print(q_tensor.shape)\n",
    "# print(map_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalcModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = (1, 120, 160)\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(5, 5), stride=2, padding=4)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(4, 4), stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "        self.conv3 = nn.Conv2d(128, 4, kernel_size=(3, 3), stride=1, padding=0)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(3, 3), stride=2)\n",
    "        self.lrn1 = nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75)\n",
    "        self.lrn2 = nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.lrn1(x)\n",
    "\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.lrn2(x)\n",
    "\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalcModel(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (conv3): Conv2d(128, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lrn1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  (lrn2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "calc = CalcModel()\n",
    "\n",
    "# Load the model weights\n",
    "state_dict = torch.load(WEIGHTS_FILE)\n",
    "my_new_state_dict = {}\n",
    "my_layers = list(calc.state_dict().keys())\n",
    "for layer in my_layers:\n",
    "    my_new_state_dict[layer] = state_dict[layer]\n",
    "calc.load_state_dict(my_new_state_dict)\n",
    "\n",
    "print(calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\onnx\\utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\onnx\\utils.py:1209: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "example_input = torch.randn(1, 1, 120, 160)\n",
    "\n",
    "dynamic_axes = {\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    calc,  # model\n",
    "    example_input,  # example input\n",
    "    \"calc_model.onnx\",  # output file name\n",
    "    input_names=[\"input\"],  # input names\n",
    "    output_names=[\"output\"],  # output names\n",
    "    dynamic_axes=dynamic_axes,  # dynamic axes\n",
    ")\n",
    "\n",
    "# Load the ONNX model\n",
    "ort_session = onnxruntime.InferenceSession(\"calc_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantized Model (ONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/lrn1/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/lrn2/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "\n",
    "model_fp32 = 'calc_model.onnx'\n",
    "model_quant = 'calc_model_quant_dynamic.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)\n",
    "\n",
    "# Load the dynamic quantized model\n",
    "ort_session_quant_dynamic = onnxruntime.InferenceSession(\"calc_model_quant_dynamic.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Quantized Model (ONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from onnxruntime.tools.symbolic_shape_infer import SymbolicShapeInference\n",
    "\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process\n",
    "\n",
    "quant_pre_process('calc_model.onnx', 'calc_model_quant_static_prep.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 120, 160])\n",
      "torch.Size([285, 1, 120, 160])\n"
     ]
    }
   ],
   "source": [
    "calib_ds = db_tensor[:100] # first 100 for calibration - reserve for quantization\n",
    "val_ds = db_tensor[100:] # last 100 for validation\n",
    "\n",
    "print(calib_ds.shape)\n",
    "print(val_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization.calibrate import CalibrationDataReader\n",
    "\n",
    "class QuantizationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, torch_ds, batch_size, input_name):\n",
    "        self.torch_dl = torch.utils.data.DataLoader(torch_ds, batch_size=batch_size, shuffle=False)\n",
    "        self.input_name = input_name\n",
    "        self.datasize = len(self.torch_dl)\n",
    "        self.enum_data = iter(self.torch_dl)\n",
    "\n",
    "    def to_numpy(self, pt_tensor):\n",
    "        return pt_tensor.detach().cpu().numpy() if pt_tensor.requires_grad else pt_tensor.cpu().numpy()\n",
    "\n",
    "    def get_next(self):\n",
    "        batch = next(self.enum_data, None)\n",
    "        if batch is not None:\n",
    "\n",
    "            data = self.to_numpy(batch[0])\n",
    "            data = np.expand_dims(data, axis=0)  # Add a new dimension to the data\n",
    "            \n",
    "            return {self.input_name: data}\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = iter(self.torch_dl)\n",
    "\n",
    "qdr = QuantizationDataReader(calib_ds, batch_size=64, input_name=ort_session.get_inputs()[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:failed to infer the type of tensor: . Skip to quantize it. Please check if it is expected.\n",
      "WARNING:root:failed to infer the type of tensor: . Skip to quantize it. Please check if it is expected.\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_static\n",
    "\n",
    "q_static_opts = {\"ActivationSymmetric\":False,\n",
    "                 \"WeightSymmetric\":True}\n",
    "# if torch.cuda.is_available():\n",
    "#     q_static_opts = {\"ActivationSymmetric\":True,\n",
    "#                   \"WeightSymmetric\":True}\n",
    "\n",
    "# q_static_opts = {\"ActivationSymmetric\":False, \"WeightSymmetric\":False}\n",
    "\n",
    "# check layer quantization support\n",
    "\n",
    "quantized_model = quantize_static(model_input='calc_model_quant_static_prep.onnx',\n",
    "                                               model_output='calc_model_quant_static.onnx',\n",
    "                                               calibration_data_reader=qdr,\n",
    "                                               extra_options=q_static_opts)\n",
    "\n",
    "# Load the static quantized model\n",
    "ort_session_quant_static = onnxruntime.InferenceSession('calc_model_quant_static.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization (Quanto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalcModel(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (conv3): Conv2d(128, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lrn1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  (lrn2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      ")\n",
      "CalcModel(\n",
      "  (conv1): QConv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): QConv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (conv3): QConv2d(128, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lrn1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  (lrn2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "calc_quanto = CalcModel()\n",
    "\n",
    "# Load the model weights\n",
    "state_dict = torch.load(WEIGHTS_FILE)\n",
    "my_new_state_dict = {}\n",
    "my_layers = list(calc.state_dict().keys())\n",
    "for layer in my_layers:\n",
    "    my_new_state_dict[layer] = state_dict[layer]\n",
    "calc_quanto.load_state_dict(my_new_state_dict)\n",
    "\n",
    "print(calc_quanto)\n",
    "\n",
    "quanto.quantize(calc_quanto, weights=quanto.qint8, activations=quanto.qint8) # quantization is in place\n",
    "print(calc_quanto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_compile = CalcModel()\n",
    "\n",
    "# # Load the model weights\n",
    "# state_dict = torch.load(WEIGHTS_FILE)\n",
    "# my_new_state_dict = {}\n",
    "# my_layers = list(calc.state_dict().keys())\n",
    "# for layer in my_layers:\n",
    "#     my_new_state_dict[layer] = state_dict[layer]\n",
    "# calc_compile.load_state_dict(my_new_state_dict)\n",
    "\n",
    "# print(calc_compile)\n",
    "\n",
    "# calc_torchscript = torch.jit.script(calc_compile)\n",
    "# print(calc_torchscript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_compile = CalcModel()\n",
    "\n",
    "# Load the model weights\n",
    "state_dict = torch.load(WEIGHTS_FILE)\n",
    "my_new_state_dict = {}\n",
    "my_layers = list(calc.state_dict().keys())\n",
    "for layer in my_layers:\n",
    "    my_new_state_dict[layer] = state_dict[layer]\n",
    "calc_compile.load_state_dict(my_new_state_dict)\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.reset()\n",
    "\n",
    "calc_compile = torch.compile(calc_compile, mode=\"reduce-overhead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "\n",
    "# Instantiate the Model\n",
    "calc_cuda = CalcModel().half().cuda()\n",
    "\n",
    "# Load the model weights\n",
    "state_dict = torch.load(WEIGHTS_FILE)\n",
    "my_new_state_dict = {}\n",
    "my_layers = list(calc_cuda.state_dict().keys())\n",
    "for layer in my_layers:\n",
    "    my_new_state_dict[layer] = state_dict[layer]\n",
    "calc_cuda.load_state_dict(my_new_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([385, 936])\n",
      "torch.Size([385, 936])\n"
     ]
    }
   ],
   "source": [
    "# query_tensor = torch.from_numpy(query_matrix)\n",
    "# map_tensor = torch.from_numpy(map_matrix)\n",
    "\n",
    "# print(query_tensor.shape)\n",
    "# print(map_tensor.shape)\n",
    "\n",
    "# db_tensor = db_tensor.view(-1, 1, 120, 160)\n",
    "# q_tensor = q_tensor.view(-1, 1, 120, 160)\n",
    "\n",
    "# Pass the tensors through the model\n",
    "\n",
    "db_features = calc(db_tensor)\n",
    "q_features = calc(q_tensor)\n",
    "\n",
    "print(db_features.shape)\n",
    "print(q_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] WON'T CONVERT forward C:\\Users\\Irfan Q\\AppData\\Local\\Temp\\ipykernel_9204\\641759170.py line 17 \n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] due to: \n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 727, in _convert_frame\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     result = inner_convert(frame, cache_entry, hooks, frame_state)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 383, in _convert_frame_assert\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_product = _compile(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                        ^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 646, in _compile\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 244, in time_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 562, in compile_inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     out_code = transform_code_object(code, transform)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1033, in transform_code_object\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     transformations(instructions, code_options)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 151, in _fn\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return fn(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 527, in transform\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     tracer.run()\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2128, in run\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     super().run()\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 818, in run\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     and self.step()\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 781, in step\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     getattr(self, inst.opname)(inst)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2243, in RETURN_VALUE\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     self.output.compile_subgraph(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 919, in compile_subgraph\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\contextlib.py\", line 81, in inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return func(*args, **kwds)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1087, in compile_and_call_fx_graph\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fn = self.call_user_compiler(gm)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 244, in time_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1159, in call_user_compiler\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1140, in call_user_compiler\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\__init__.py\", line 1668, in __call__\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 952, in compile_fx\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return compile_fx(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1168, in compile_fx\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return aot_autograd(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 55, in compiler_fn\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 887, in aot_module_simplified\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fn = create_aot_dispatcher_function(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 244, in time_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 600, in create_aot_dispatcher_function\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 425, in aot_wrapper_dedupe\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 630, in aot_wrapper_synthetic_base\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 295, in aot_dispatch_autograd\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 244, in time_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1100, in fw_compiler_base\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return inner_compile(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\contextlib.py\", line 81, in inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return func(*args, **kwds)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 305, in inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return fn(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\contextlib.py\", line 81, in inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return func(*args, **kwds)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 320, in compile_fx_inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_graph = fx_codegen_and_compile(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 550, in fx_codegen_and_compile\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fn = graph.compile_to_fn()\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1116, in compile_to_fn\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return self.compile_to_module().call\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 244, in time_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1070, in compile_to_module\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     mod = PyCodeCache.load_by_key_path(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1886, in load_by_key_path\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     raise RuntimeError(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] RuntimeError: Failed to import C:\\Users\\IRFANQ~1\\AppData\\Local\\Temp\\torchinductor_Irfan Q\\pf\\cpfhzu3wc4gbgtetzmtmv4fw3tizgdncz5iec55zg6a3gwoagq63.py\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 13-14: truncated \\UXXXXXXXX escape (cpfhzu3wc4gbgtetzmtmv4fw3tizgdncz5iec55zg6a3gwoagq63.py, line 71)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] \n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] \n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 727, in _convert_frame\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     result = inner_convert(frame, cache_entry, hooks, frame_state)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 383, in _convert_frame_assert\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_product = _compile(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                        ^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 646, in _compile\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 244, in time_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 562, in compile_inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     out_code = transform_code_object(code, transform)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1033, in transform_code_object\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     transformations(instructions, code_options)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 151, in _fn\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return fn(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 527, in transform\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     tracer.run()\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2128, in run\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     super().run()\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 818, in run\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     and self.step()\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 781, in step\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     getattr(self, inst.opname)(inst)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2243, in RETURN_VALUE\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     self.output.compile_subgraph(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 919, in compile_subgraph\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\contextlib.py\", line 81, in inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return func(*args, **kwds)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1087, in compile_and_call_fx_graph\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fn = self.call_user_compiler(gm)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 244, in time_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1159, in call_user_compiler\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1140, in call_user_compiler\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\__init__.py\", line 1668, in __call__\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 952, in compile_fx\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return compile_fx(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1168, in compile_fx\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return aot_autograd(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 55, in compiler_fn\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 887, in aot_module_simplified\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fn = create_aot_dispatcher_function(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 244, in time_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 600, in create_aot_dispatcher_function\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 425, in aot_wrapper_dedupe\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 630, in aot_wrapper_synthetic_base\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 295, in aot_dispatch_autograd\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 244, in time_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1100, in fw_compiler_base\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return inner_compile(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\contextlib.py\", line 81, in inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return func(*args, **kwds)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 305, in inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return fn(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\contextlib.py\", line 81, in inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return func(*args, **kwds)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 320, in compile_fx_inner\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_graph = fx_codegen_and_compile(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 550, in fx_codegen_and_compile\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     compiled_fn = graph.compile_to_fn()\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1116, in compile_to_fn\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     return self.compile_to_module().call\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 244, in time_wrapper\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1070, in compile_to_module\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     mod = PyCodeCache.load_by_key_path(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]   File \"c:\\Users\\Irfan Q\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1886, in load_by_key_path\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING]     raise RuntimeError(\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] RuntimeError: Failed to import C:\\Users\\IRFANQ~1\\AppData\\Local\\Temp\\torchinductor_Irfan Q\\pf\\cpfhzu3wc4gbgtetzmtmv4fw3tizgdncz5iec55zg6a3gwoagq63.py\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 13-14: truncated \\UXXXXXXXX escape (cpfhzu3wc4gbgtetzmtmv4fw3tizgdncz5iec55zg6a3gwoagq63.py, line 71)\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] \n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "[2024-04-16 14:02:50,643] torch._dynamo.convert_frame: [WARNING] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([385, 936])\n",
      "torch.Size([385, 936])\n"
     ]
    }
   ],
   "source": [
    "#idk why this is needed\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "db_features_comp = calc_compile(db_tensor)\n",
    "q_features_comp = calc_compile(q_tensor)\n",
    "\n",
    "print(db_features_comp.shape)\n",
    "print(q_features_comp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model is a valid ONNX model\n",
    "onnx_model = onnx.load(\"calc_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tensors to numpy arrays\n",
    "db_matrix = db_tensor.detach().cpu().numpy()\n",
    "q_matrix = q_tensor.detach().cpu().numpy()\n",
    "\n",
    "# ort_session = onnxruntime.InferenceSession(\"calc_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 385, 936)\n",
      "(1, 385, 936)\n"
     ]
    }
   ],
   "source": [
    "# Get the input name from the model\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "# Ensure the inputs are numpy arrays\n",
    "db_matrix = np.array(db_matrix)\n",
    "q_matrix = np.array(q_matrix)\n",
    "\n",
    "## Database images\n",
    "\n",
    "# Create the input dictionary\n",
    "ort_db_input = {input_name: db_matrix}\n",
    "\n",
    "# Run the model\n",
    "ort_db_output = ort_session.run(None, ort_db_input)\n",
    "\n",
    "# Convert the output to a numpy array and print its shape\n",
    "ort_db_output = np.array(ort_db_output)\n",
    "print(ort_db_output.shape)\n",
    "\n",
    "## Query images\n",
    "\n",
    "# Create the input dictionary\n",
    "ort_q_input = {input_name: q_matrix}\n",
    "\n",
    "# Run the model\n",
    "ort_q_output = ort_session.run(None, ort_q_input)\n",
    "\n",
    "# Convert the output to a numpy array and print its shape\n",
    "ort_q_output = np.array(ort_q_output)\n",
    "print(ort_q_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(385, 936)\n",
      "(385, 936)\n"
     ]
    }
   ],
   "source": [
    "ort_db_output = np.squeeze(ort_db_output)\n",
    "print(ort_db_output.shape)\n",
    "ort_q_output = np.squeeze(ort_q_output)\n",
    "print(ort_q_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantized Model (ONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model is a valid ONNX model\n",
    "onnx_model_quant_dynamic = onnx.load(\"calc_model_quant_dynamic.onnx\")\n",
    "onnx.checker.check_model(onnx_model_quant_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session_quant_dynamic = onnxruntime.InferenceSession(\"calc_model_quant_dynamic.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 385, 936)\n",
      "(1, 385, 936)\n"
     ]
    }
   ],
   "source": [
    "# Get the input name from the model\n",
    "input_name_quant_dynamic = ort_session_quant_dynamic.get_inputs()[0].name\n",
    "\n",
    "# Ensure the inputs are numpy arrays\n",
    "db_matrix = np.array(db_matrix)\n",
    "q_matrix = np.array(q_matrix)\n",
    "\n",
    "## Database images\n",
    "\n",
    "# Create the input dictionary\n",
    "ort_db_input_quant_dynamic = {input_name: db_matrix}\n",
    "\n",
    "# Run the model\n",
    "ort_db_output_quant_dynamic = ort_session_quant_dynamic.run(None, ort_db_input_quant_dynamic)\n",
    "\n",
    "# Convert the output to a numpy array and print its shape\n",
    "ort_db_output_quant_dynamic = np.array(ort_db_output_quant_dynamic)\n",
    "print(ort_db_output_quant_dynamic.shape)\n",
    "\n",
    "## Query images\n",
    "\n",
    "# Create the input dictionary\n",
    "ort_q_input_quant_dynamic = {input_name: q_matrix}\n",
    "\n",
    "# Run the model\n",
    "ort_q_output_quant_dynamic = ort_session_quant_dynamic.run(None, ort_q_input_quant_dynamic)\n",
    "\n",
    "# Convert the output to a numpy array and print its shape\n",
    "ort_q_output_quant_dynamic = np.array(ort_q_output_quant_dynamic)\n",
    "print(ort_q_output_quant_dynamic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Quantized Model (ONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model is a valid ONNX model\n",
    "onnx_model_quant_static = onnx.load(\"calc_model_quant_static.onnx\")\n",
    "onnx.checker.check_model(onnx_model_quant_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session_quant_static = onnxruntime.InferenceSession(\"calc_model_quant_static.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 385, 936)\n",
      "(1, 385, 936)\n"
     ]
    }
   ],
   "source": [
    "# Get the input name from the model\n",
    "input_name_quant_static = ort_session_quant_static.get_inputs()[0].name\n",
    "\n",
    "# Ensure the inputs are numpy arrays\n",
    "db_matrix = np.array(db_matrix)\n",
    "q_matrix = np.array(q_matrix)\n",
    "\n",
    "## Database images\n",
    "\n",
    "# Create the input dictionary\n",
    "ort_db_input_quant_static = {input_name: db_matrix}\n",
    "\n",
    "# Run the model\n",
    "ort_db_output_quant_static = ort_session_quant_static.run(None, ort_db_input_quant_static)\n",
    "\n",
    "# Convert the output to a numpy array and print its shape\n",
    "ort_db_output_quant_static = np.array(ort_db_output_quant_static)\n",
    "print(ort_db_output_quant_static.shape)\n",
    "\n",
    "## Query images\n",
    "\n",
    "# Create the input dictionary\n",
    "ort_q_input_quant_static = {input_name: q_matrix}\n",
    "\n",
    "# Run the model\n",
    "ort_q_output_quant_static = ort_session_quant_static.run(None, ort_q_input_quant_static)\n",
    "\n",
    "# Convert the output to a numpy array and print its shape\n",
    "ort_q_output_quant_static = np.array(ort_q_output_quant_static)\n",
    "print(ort_q_output_quant_static.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([385, 936])\n",
      "torch.Size([385, 936])\n"
     ]
    }
   ],
   "source": [
    "# Pass the tensors through the model\n",
    "\n",
    "db_features_cuda = calc_cuda(db_tensor.half().cuda())\n",
    "q_features_cuda = calc_cuda(q_tensor.half().cuda())\n",
    "\n",
    "db_features_cuda = db_features_cuda.detach().cpu()\n",
    "q_features_cuda = q_features_cuda.detach().cpu()\n",
    "\n",
    "print(db_features_cuda.shape)\n",
    "print(q_features_cuda.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 51.079155683517456 seconds\n",
      "Average time taken: 0.5107915568351745 seconds\n",
      "Time taken: 54.9536190032959 seconds\n",
      "Average time taken: 0.549536190032959 seconds\n"
     ]
    }
   ],
   "source": [
    "# Average time for database tensor\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    output = calc(db_tensor)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "db_time = end_time - start_time\n",
    "db_avg_time = db_time / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {db_time} seconds\")\n",
    "print(f\"Average time taken: {db_avg_time} seconds\")\n",
    "\n",
    "# Average time for map tensor\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    with torch.no_grad():\n",
    "        output = calc(q_tensor)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "q_time = end_time - start_time\n",
    "q_avg_time = q_time / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {q_time} seconds\")\n",
    "print(f\"Average time taken: {q_avg_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 52.05290198326111 seconds\n",
      "Average time taken: 0.5205290198326111 seconds\n",
      "Time taken: 55.157925605773926 seconds\n",
      "Average time taken: 0.5515792560577393 seconds\n"
     ]
    }
   ],
   "source": [
    "# Average time for database tensor\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    output = calc_compile(db_tensor)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "db_time = end_time - start_time\n",
    "db_avg_time = db_time / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {db_time} seconds\")\n",
    "print(f\"Average time taken: {db_avg_time} seconds\")\n",
    "\n",
    "# Average time for map tensor\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    with torch.no_grad():\n",
    "        output = calc_compile(q_tensor)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "q_time = end_time - start_time\n",
    "q_avg_time = q_time / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {q_time} seconds\")\n",
    "print(f\"Average time taken: {q_avg_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 25.50274133682251 seconds\n",
      "Average time taken: 0.2550274133682251 seconds\n",
      "Time taken: 25.321824073791504 seconds\n",
      "Average time taken: 0.25321824073791505 seconds\n"
     ]
    }
   ],
   "source": [
    "# Average time for database tensor\n",
    "\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "ort_inputs = {input_name: db_matrix}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "db_time_onnx = end_time - start_time\n",
    "db_avg_time_onnx = db_time_onnx / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {db_time_onnx} seconds\")\n",
    "print(f\"Average time taken: {db_avg_time_onnx} seconds\")\n",
    "\n",
    "# Average time for query tensor\n",
    "\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "ort_inputs = {input_name: q_matrix}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "q_time_onnx = end_time - start_time\n",
    "q_avg_time_onnx = q_time_onnx / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {q_time_onnx} seconds\")\n",
    "print(f\"Average time taken: {q_avg_time_onnx} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantized Model (ONNX)\n",
    "Bad Results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 59.28674650192261 seconds\n",
      "Average time taken: 0.592867465019226 seconds\n",
      "Time taken: 59.04523968696594 seconds\n",
      "Average time taken: 0.5904523968696594 seconds\n"
     ]
    }
   ],
   "source": [
    "# Average time for database tensor\n",
    "\n",
    "input_name_quant_dynamic = ort_session_quant_dynamic.get_inputs()[0].name\n",
    "ort_inputs_quant_dynamic = {input_name_quant_dynamic: db_matrix}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    ort_outs_quant_dynamic = ort_session_quant_dynamic.run(None, ort_inputs_quant_dynamic)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "db_time_onnx_quant_dynamic = end_time - start_time\n",
    "db_avg_time_onnx_quant_dynamic = db_time_onnx_quant_dynamic / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {db_time_onnx_quant_dynamic} seconds\")\n",
    "print(f\"Average time taken: {db_avg_time_onnx_quant_dynamic} seconds\")\n",
    "\n",
    "# Average time for query tensor\n",
    "\n",
    "input_name_quant_dynamic = ort_session_quant_dynamic.get_inputs()[0].name\n",
    "ort_inputs_quant_dynamic = {input_name_quant_dynamic: q_matrix}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    ort_outs_quant_dynamic = ort_session_quant_dynamic.run(None, ort_inputs)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "q_time_onnx_quant_dynamic = end_time - start_time\n",
    "q_avg_time_onnx_quant_dynamic = q_time_onnx_quant_dynamic / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {q_time_onnx_quant_dynamic} seconds\")\n",
    "print(f\"Average time taken: {q_avg_time_onnx_quant_dynamic} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 44.940831422805786 seconds\n",
      "Average time taken: 0.4494083142280579 seconds\n",
      "Time taken: 44.68128848075867 seconds\n",
      "Average time taken: 0.4468128848075867 seconds\n"
     ]
    }
   ],
   "source": [
    "# Average time for database tensor\n",
    "\n",
    "input_name_quant_static = ort_session_quant_static.get_inputs()[0].name\n",
    "ort_inputs_quant_static = {input_name_quant_static: db_matrix}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    ort_outs_quant_static = ort_session_quant_static.run(None, ort_inputs_quant_static)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "db_time_onnx_quant_static = end_time - start_time\n",
    "db_avg_time_onnx_quant_static = db_time_onnx_quant_static / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {db_time_onnx_quant_static} seconds\")\n",
    "print(f\"Average time taken: {db_avg_time_onnx_quant_static} seconds\")\n",
    "\n",
    "# Average time for query tensor\n",
    "\n",
    "input_name_quant_static = ort_session_quant_static.get_inputs()[0].name\n",
    "ort_inputs_quant_static = {input_name_quant_static: q_matrix}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    ort_outs_quant_static = ort_session_quant_static.run(None, ort_inputs)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "q_time_onnx_quant_static = end_time - start_time\n",
    "q_avg_time_onnx_quant_static = q_time_onnx_quant_static / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {q_time_onnx_quant_static} seconds\")\n",
    "print(f\"Average time taken: {q_avg_time_onnx_quant_static} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3.0664443969726562 seconds\n",
      "Average time taken: 0.030664443969726562 seconds\n",
      "Time taken: 2.800022602081299 seconds\n",
      "Average time taken: 0.02800022602081299 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Average time for database tensor\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    output = calc_cuda(db_tensor.half().cuda())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "db_time_cuda = end_time - start_time\n",
    "db_avg_time_cuda = db_time_cuda / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {db_time_cuda} seconds\")\n",
    "print(f\"Average time taken: {db_avg_time_cuda} seconds\")\n",
    "\n",
    "# Average time for query tensor\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    output = calc_cuda(q_tensor.half().cuda())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "q_time_cuda = end_time - start_time\n",
    "q_avg_time_cuda = q_time_cuda / ITERATIONS\n",
    "\n",
    "print(f\"Time taken: {q_time_cuda} seconds\")\n",
    "print(f\"Average time taken: {q_avg_time_cuda} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Variables\n",
    "models = ['Normal', 'ONNX', 'CUDA', 'Dynamic\\n Quantization\\n (ONNX)', 'Static\\n Quantization\\n (ONNX)']\n",
    "durations = [db_time, db_time_onnx, db_time_cuda, db_time_onnx_quant_dynamic, db_time_onnx_quant_static]\n",
    "avg_durations = [db_avg_time, db_avg_time_onnx, db_avg_time_cuda, db_avg_time_onnx_quant_dynamic, db_avg_time_onnx_quant_static]\n",
    "\n",
    "# Plot duration bar chart\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.bar(models, durations)\n",
    "plt.title('Duration')\n",
    "plt.ylabel('Duration')\n",
    "plt.show()\n",
    "\n",
    "# Plot average duration bar chart\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.bar(models, avg_durations)\n",
    "plt.title('Average Duration')\n",
    "plt.ylabel('Average Duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision-recall curve\n",
    "# P, R = createPR(similarity_matrix, GThard, GTsoft, matching='multi', n_thresh=100)\n",
    "# plt.figure()\n",
    "# plt.plot(R, P)\n",
    "# plt.xlim(0, 1), plt.ylim(0, 1.01)\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Result on GardensPoint day_right--night_right')\n",
    "# plt.grid('on')\n",
    "# plt.draw()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vprtutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
