{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (2.3.0+cu118)\n",
      "Requirement already satisfied: torchvision in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (0.18.0+cu118)\n",
      "Requirement already satisfied: torchaudio in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (2.3.0+cu118)\n",
      "Requirement already satisfied: filelock in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch) (2.3.0)\n",
      "Requirement already satisfied: numpy in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (1.26.3)\n",
      "Requirement already satisfied: seaborn in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (3.8.4)\n",
      "Requirement already satisfied: scikit-learn in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (1.4.2)\n",
      "Requirement already satisfied: ipykernel in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (6.29.3)\n",
      "Requirement already satisfied: tqdm in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (4.66.2)\n",
      "Requirement already satisfied: pillow in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (10.2.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (8.22.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: psutil in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: onnx in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (1.16.0)\n",
      "Requirement already satisfied: onnxruntime in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (1.17.3)\n",
      "Requirement already satisfied: quanto in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (0.1.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from onnx) (1.26.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from onnx) (5.26.1)\n",
      "Requirement already satisfied: coloredlogs in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: packaging in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from onnxruntime) (24.0)\n",
      "Requirement already satisfied: sympy in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: torch>=2.2.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from quanto) (2.3.0+cu118)\n",
      "Requirement already satisfied: ninja in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from quanto) (1.11.1.1)\n",
      "Requirement already satisfied: safetensors in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from quanto) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (4.11.0)\n",
      "Requirement already satisfied: networkx in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (11.8.86)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from torch>=2.2.0->quanto) (2.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/irfan/.conda/envs/bruh/lib/python3.11/site-packages (from jinja2->torch>=2.2.0->quanto) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install opencv-python numpy seaborn matplotlib scikit-learn ipykernel tqdm pillow\n",
    "%pip install onnx onnxruntime quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import quanto\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "\n",
    "from datasets import dataset_utils\n",
    "from matching import matching\n",
    "from evaluation.metrics import createPR, recallAt100precision, recallAtK\n",
    "from datasets.load_dataset import GardensPointDataset, SFUDataset, StLuciaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GardensPointDataset().load()\n",
    "# SFUDataset().load()\n",
    "# StLuciaDataset().load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_FILE = \"calc.caffemodel.pt\"\n",
    "ITERATIONS = 100 # for testing average duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToYUVandEqualizeHist:\n",
    "    def __call__(self, img):\n",
    "        img_yuv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2YUV)\n",
    "        img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :, 0])\n",
    "        img_rgb = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)\n",
    "        return Image.fromarray(img_rgb)\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        ConvertToYUVandEqualizeHist(),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((120, 160), interpolation=Image.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, name, folder, transform=None):\n",
    "        \n",
    "        self.name = os.path.basename(name)\n",
    "        self.folder = os.path.join(name, folder)\n",
    "        self.image_paths = dataset_utils.read_images_paths(self.folder, get_abs_path=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        image_path = self.image_paths[index]\n",
    "        img = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length: 385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3098, 0.4431, 0.6235,  ..., 0.0314, 0.0196, 0.0196],\n",
       "         [0.1882, 0.4471, 0.7020,  ..., 0.0471, 0.0353, 0.0353],\n",
       "         [0.1412, 0.4392, 0.6510,  ..., 0.0431, 0.0314, 0.0353],\n",
       "         ...,\n",
       "         [0.7882, 0.8157, 0.8314,  ..., 0.1529, 0.1176, 0.0824],\n",
       "         [0.7961, 0.8118, 0.8353,  ..., 0.1333, 0.0902, 0.0627],\n",
       "         [0.7843, 0.8000, 0.8196,  ..., 0.0980, 0.0588, 0.0431]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_db = CustomImageDataset(\"images/SFU\", \"dry\", preprocess)\n",
    "dataset_q = CustomImageDataset(\"images/SFU\", \"jan\", preprocess)\n",
    "\n",
    "print(\"Dataset Length:\", len(dataset_db))\n",
    "dataset_db[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 8\n",
    "db_dataloader = DataLoader(dataset_db, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "q_dataloader = DataLoader(dataset_q, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalcModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = (1, 120, 160)\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(5, 5), stride=2, padding=4)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(4, 4), stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "        self.conv3 = nn.Conv2d(128, 4, kernel_size=(3, 3), stride=1, padding=0)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(3, 3), stride=2)\n",
    "        self.lrn1 = nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75)\n",
    "        self.lrn2 = nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.lrn1(x)\n",
    "\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.lrn2(x)\n",
    "\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalcModelCompile(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = (1, 120, 160)\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(5, 5), stride=2, padding=4)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(4, 4), stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "        self.conv3 = nn.Conv2d(128, 4, kernel_size=(3, 3), stride=1, padding=0)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(3, 3), stride=2)\n",
    "        self.lrn1 = nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75)\n",
    "        self.lrn2 = nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75)\n",
    "\n",
    "    @torch.compile\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.lrn1(x)\n",
    "\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.lrn2(x)\n",
    "\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalcModel(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (conv3): Conv2d(128, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lrn1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  (lrn2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "calc = CalcModel()\n",
    "\n",
    "# Load the model weights\n",
    "state_dict = torch.load(WEIGHTS_FILE)\n",
    "my_new_state_dict = {}\n",
    "my_layers = list(calc.state_dict().keys())\n",
    "for layer in my_layers:\n",
    "    my_new_state_dict[layer] = state_dict[layer]\n",
    "calc.load_state_dict(my_new_state_dict)\n",
    "\n",
    "print(calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/.conda/envs/bruh/lib/python3.11/site-packages/torch/onnx/_internal/jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/irfan/.conda/envs/bruh/lib/python3.11/site-packages/torch/onnx/utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/irfan/.conda/envs/bruh/lib/python3.11/site-packages/torch/onnx/utils.py:1208: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "example_input = torch.randn(1, 1, 120, 160)\n",
    "\n",
    "dynamic_axes = {\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    calc,  # model\n",
    "    example_input,  # example input\n",
    "    \"calc_model.onnx\",  # output file name\n",
    "    input_names=[\"input\"],  # input names\n",
    "    output_names=[\"output\"],  # output names\n",
    "    dynamic_axes=dynamic_axes,  # dynamic axes\n",
    ")\n",
    "\n",
    "# Load the ONNX model\n",
    "ort_session = onnxruntime.InferenceSession(\"calc_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantized Model (ONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/lrn1/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/lrn2/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'calc_model.onnx'\n",
    "model_quant = 'calc_model_quant_dynamic.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)\n",
    "\n",
    "# Load the dynamic quantized model\n",
    "ort_session_quant_dynamic = onnxruntime.InferenceSession(\"calc_model_quant_dynamic.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Quantized Model (ONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from onnxruntime.tools.symbolic_shape_infer import SymbolicShapeInference\n",
    "\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process\n",
    "\n",
    "quant_pre_process('calc_model.onnx', 'calc_model_quant_static_prep.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 120, 160])\n",
      "torch.Size([285, 1, 120, 160])\n"
     ]
    }
   ],
   "source": [
    "# calib_ds = db_tensor[:100] # first 100 for calibration - reserve for quantization\n",
    "# val_ds = db_tensor[100:] # last 100 for validation\n",
    "\n",
    "calib_ds = torch.stack([dataset_db[i] for i in range(100)])\n",
    "val_ds = torch.stack([dataset_db[i] for i in range(100, len(dataset_db))])\n",
    "\n",
    "print(calib_ds.shape)\n",
    "print(val_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization.calibrate import CalibrationDataReader\n",
    "\n",
    "class QuantizationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, torch_ds, batch_size, input_name):\n",
    "        self.torch_dl = torch.utils.data.DataLoader(torch_ds, batch_size=batch_size, shuffle=False)\n",
    "        self.input_name = input_name\n",
    "        self.datasize = len(self.torch_dl)\n",
    "        self.enum_data = iter(self.torch_dl)\n",
    "\n",
    "    def to_numpy(self, pt_tensor):\n",
    "        return pt_tensor.detach().cpu().numpy() if pt_tensor.requires_grad else pt_tensor.cpu().numpy()\n",
    "\n",
    "    def get_next(self):\n",
    "        batch = next(self.enum_data, None)\n",
    "        if batch is not None:\n",
    "\n",
    "            data = self.to_numpy(batch[0])\n",
    "            data = np.expand_dims(data, axis=0)  # Add a new dimension to the data\n",
    "            \n",
    "            return {self.input_name: data}\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = iter(self.torch_dl)\n",
    "\n",
    "qdr = QuantizationDataReader(calib_ds, batch_size=64, input_name=ort_session.get_inputs()[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:failed to infer the type of tensor: . Skip to quantize it. Please check if it is expected.\n",
      "WARNING:root:failed to infer the type of tensor: . Skip to quantize it. Please check if it is expected.\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_static\n",
    "\n",
    "q_static_opts = {\"ActivationSymmetric\":False,\n",
    "                 \"WeightSymmetric\":True}\n",
    "# if torch.cuda.is_available():\n",
    "#     q_static_opts = {\"ActivationSymmetric\":True,\n",
    "#                   \"WeightSymmetric\":True}\n",
    "\n",
    "# q_static_opts = {\"ActivationSymmetric\":False, \"WeightSymmetric\":False}\n",
    "\n",
    "# check layer quantization support\n",
    "\n",
    "quantized_model = quantize_static(model_input='calc_model_quant_static_prep.onnx',\n",
    "                                               model_output='calc_model_quant_static.onnx',\n",
    "                                               calibration_data_reader=qdr,\n",
    "                                               extra_options=q_static_opts)\n",
    "\n",
    "# Load the static quantized model\n",
    "ort_session_quant_static = onnxruntime.InferenceSession('calc_model_quant_static.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization (Quanto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalcModel(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (conv3): Conv2d(128, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lrn1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  (lrn2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      ")\n",
      "CalcModel(\n",
      "  (conv1): QConv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): QConv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (conv3): QConv2d(128, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lrn1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  (lrn2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "calc_quanto = CalcModel()\n",
    "\n",
    "# Load the model weights\n",
    "state_dict = torch.load(WEIGHTS_FILE)\n",
    "my_new_state_dict = {}\n",
    "my_layers = list(calc.state_dict().keys())\n",
    "for layer in my_layers:\n",
    "    my_new_state_dict[layer] = state_dict[layer]\n",
    "calc_quanto.load_state_dict(my_new_state_dict)\n",
    "\n",
    "print(calc_quanto)\n",
    "\n",
    "quanto.quantize(calc_quanto, weights=quanto.qint8, activations=quanto.qint8) # quantization is in place\n",
    "print(calc_quanto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): CalcModel(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv2d(128, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (relu3): ReLU()\n",
      "    (pool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (lrn1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "    (lrn2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "calc_compiled = torch.compile(calc, mode='default')\n",
    "\n",
    "print(calc_compiled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([385, 936])\n",
      "torch.Size([385, 936])\n"
     ]
    }
   ],
   "source": [
    "calc.eval()\n",
    "\n",
    "# Pass database tensor through the model\n",
    "\n",
    "db_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in db_dataloader:\n",
    "        output = calc(batch)\n",
    "        db_features.append(output)\n",
    "\n",
    "db_features = torch.cat(db_features, axis=0)\n",
    "\n",
    "print(db_features.shape)\n",
    "\n",
    "# Pass query tensor through the model\n",
    "\n",
    "q_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in q_dataloader:\n",
    "        output = calc(batch)\n",
    "        q_features.append(output)\n",
    "\n",
    "q_features = torch.cat(q_features, axis=0)\n",
    "\n",
    "print(q_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model is a valid ONNX model\n",
    "onnx_model = onnx.load(\"calc_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the input name from the model\n",
    "# input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "# # Ensure the inputs are numpy arrays\n",
    "# db_matrix = np.array(db_matrix)\n",
    "# q_matrix = np.array(q_matrix)\n",
    "\n",
    "# ## Database images\n",
    "\n",
    "# # Create the input dictionary\n",
    "# ort_db_input = {input_name: db_matrix}\n",
    "\n",
    "# # Run the model\n",
    "# ort_db_output = ort_session.run(None, ort_db_input)\n",
    "\n",
    "# # Convert the output to a numpy array and print its shape\n",
    "# ort_db_output = np.array(ort_db_output)\n",
    "# print(ort_db_output.shape)\n",
    "\n",
    "# ## Query images\n",
    "\n",
    "# # Create the input dictionary\n",
    "# ort_q_input = {input_name: q_matrix}\n",
    "\n",
    "# # Run the model\n",
    "# ort_q_output = ort_session.run(None, ort_q_input)\n",
    "\n",
    "# # Convert the output to a numpy array and print its shape\n",
    "# ort_q_output = np.array(ort_q_output)\n",
    "# print(ort_q_output.shape)\n",
    "\n",
    "### USE NUMPY ARRAYS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30980393 0.44313726 0.62352943 ... 0.03137255 0.01960784 0.01960784]\n",
      " [0.1882353  0.44705883 0.7019608  ... 0.04705882 0.03529412 0.03529412]\n",
      " [0.14117648 0.4392157  0.6509804  ... 0.04313726 0.03137255 0.03529412]\n",
      " ...\n",
      " [0.7882353  0.8156863  0.83137256 ... 0.15294118 0.11764706 0.08235294]\n",
      " [0.79607844 0.8117647  0.8352941  ... 0.13333334 0.09019608 0.0627451 ]\n",
      " [0.78431374 0.8        0.81960785 ... 0.09803922 0.05882353 0.04313726]]\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: input Got: 3 Expected: 4 Please fix either the inputs/outputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(ort_db_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m ort_db_output \u001b[38;5;241m=\u001b[39m \u001b[43mort_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mort_db_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert the output to a numpy array and print its shape\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ort_db_output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(ort_db_output)\n",
      "File \u001b[0;32m~/.conda/envs/bruh/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: input Got: 3 Expected: 4 Please fix either the inputs/outputs or the model."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Get the input name from the model\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "## Database images\n",
    "\n",
    "for db_inputs in db_dataloader:\n",
    "    # Create the input dictionary\n",
    "    # ort_db_input = {input_name: db_inputs[0].detach().cpu().numpy()}\n",
    "\n",
    "    # print(ort_db_input['input'][0])\n",
    "\n",
    "    # Run the model\n",
    "    ort_db_output = ort_session.run(None, ort_db_input)\n",
    "\n",
    "    # Convert the output to a numpy array and print its shape\n",
    "    ort_db_output = np.array(ort_db_output)\n",
    "    print(ort_db_output.shape)\n",
    "\n",
    "## Query images\n",
    "\n",
    "for q_inputs in q_dataloader:\n",
    "    # Create the input dictionary\n",
    "    ort_q_input = {input_name: q_inputs[0].detach().cpu().numpy()}\n",
    "\n",
    "    # Run the model\n",
    "    ort_q_output = ort_session.run(None, ort_q_input)\n",
    "\n",
    "    # Convert the output to a numpy array and print its shape\n",
    "    ort_q_output = np.array(ort_q_output)\n",
    "    print(ort_q_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_db_output = np.squeeze(ort_db_output)\n",
    "print(ort_db_output.shape)\n",
    "ort_q_output = np.squeeze(ort_q_output)\n",
    "print(ort_q_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantized Model (ONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model is a valid ONNX model\n",
    "onnx_model_quant_dynamic = onnx.load(\"calc_model_quant_dynamic.onnx\")\n",
    "onnx.checker.check_model(onnx_model_quant_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session_quant_dynamic = onnxruntime.InferenceSession(\"calc_model_quant_dynamic.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input name from the model\n",
    "input_name_quant_dynamic = ort_session_quant_dynamic.get_inputs()[0].name\n",
    "\n",
    "# Ensure the inputs are numpy arrays\n",
    "db_matrix = np.array(db_matrix)\n",
    "q_matrix = np.array(q_matrix)\n",
    "\n",
    "## Database images\n",
    "\n",
    "# Create the input dictionary\n",
    "ort_db_input_quant_dynamic = {input_name: db_matrix}\n",
    "\n",
    "# Run the model\n",
    "ort_db_output_quant_dynamic = ort_session_quant_dynamic.run(None, ort_db_input_quant_dynamic)\n",
    "\n",
    "# Convert the output to a numpy array and print its shape\n",
    "ort_db_output_quant_dynamic = np.array(ort_db_output_quant_dynamic)\n",
    "print(ort_db_output_quant_dynamic.shape)\n",
    "\n",
    "## Query images\n",
    "\n",
    "# Create the input dictionary\n",
    "ort_q_input_quant_dynamic = {input_name: q_matrix}\n",
    "\n",
    "# Run the model\n",
    "ort_q_output_quant_dynamic = ort_session_quant_dynamic.run(None, ort_q_input_quant_dynamic)\n",
    "\n",
    "# Convert the output to a numpy array and print its shape\n",
    "ort_q_output_quant_dynamic = np.array(ort_q_output_quant_dynamic)\n",
    "print(ort_q_output_quant_dynamic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Quantized Model (ONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model is a valid ONNX model\n",
    "onnx_model_quant_static = onnx.load(\"calc_model_quant_static.onnx\")\n",
    "onnx.checker.check_model(onnx_model_quant_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session_quant_static = onnxruntime.InferenceSession(\"calc_model_quant_static.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input name from the model\n",
    "input_name_quant_static = ort_session_quant_static.get_inputs()[0].name\n",
    "\n",
    "# Ensure the inputs are numpy arrays\n",
    "db_matrix = np.array(db_matrix)\n",
    "q_matrix = np.array(q_matrix)\n",
    "\n",
    "## Database images\n",
    "\n",
    "# Create the input dictionary\n",
    "ort_db_input_quant_static = {input_name: db_matrix}\n",
    "\n",
    "# Run the model\n",
    "ort_db_output_quant_static = ort_session_quant_static.run(None, ort_db_input_quant_static)\n",
    "\n",
    "# Convert the output to a numpy array and print its shape\n",
    "ort_db_output_quant_static = np.array(ort_db_output_quant_static)\n",
    "print(ort_db_output_quant_static.shape)\n",
    "\n",
    "## Query images\n",
    "\n",
    "# Create the input dictionary\n",
    "ort_q_input_quant_static = {input_name: q_matrix}\n",
    "\n",
    "# Run the model\n",
    "ort_q_output_quant_static = ort_session_quant_static.run(None, ort_q_input_quant_static)\n",
    "\n",
    "# Convert the output to a numpy array and print its shape\n",
    "ort_q_output_quant_static = np.array(ort_q_output_quant_static)\n",
    "print(ort_q_output_quant_static.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization (Quanto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_quanto.eval()\n",
    "\n",
    "# Pass database tensor through the model\n",
    "\n",
    "db_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in db_dataloader:\n",
    "        output = calc_quanto(batch)\n",
    "        db_features.append(output)\n",
    "\n",
    "db_features = torch.cat(db_features, axis=0)\n",
    "\n",
    "print(db_features.shape)\n",
    "\n",
    "# Pass query tensor through the model\n",
    "\n",
    "q_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in q_dataloader:\n",
    "        output = calc_quanto(batch)\n",
    "        q_features.append(output)\n",
    "\n",
    "q_features = torch.cat(q_features, axis=0)\n",
    "\n",
    "print(q_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_compiled.eval()\n",
    "\n",
    "# Pass database tensor through the model\n",
    "\n",
    "db_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in db_dataloader:\n",
    "        output = calc_compiled(batch)\n",
    "        db_features.append(output)\n",
    "\n",
    "db_features = torch.cat(db_features, axis=0)\n",
    "\n",
    "print(db_features.shape)\n",
    "\n",
    "# Pass query tensor through the model\n",
    "\n",
    "q_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in q_dataloader:\n",
    "        output = calc_compiled(batch)\n",
    "        q_features.append(output)\n",
    "\n",
    "q_features = torch.cat(q_features, axis=0)\n",
    "\n",
    "print(q_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [] # Initialize a list to store the time for each pass\n",
    "\n",
    "for _ in tqdm(range(ITERATIONS), desc=\"Processing database dataset\"):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Pass the dataset through the model\n",
    "    with torch.no_grad():\n",
    "        db_features = []\n",
    "        for batch in db_dataloader:\n",
    "            output = calc(batch)\n",
    "\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "average_time = sum(times) / len(times) # Calculate the average time\n",
    "\n",
    "print(f'Average time: {average_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [] # Initialize a list to store the time for each pass\n",
    "\n",
    "for _ in tqdm(range(ITERATIONS), desc=\"Processing query dataset\"):\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in q_dataloader:\n",
    "            output = calc(batch)\n",
    "\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "average_time = sum(times) / len(times) # Calculate the average time\n",
    "\n",
    "print(f'Average time: {average_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [] # Initialize a list to store the time for each pass\n",
    "\n",
    "for _ in tqdm(range(ITERATIONS), desc=\"Processing database dataset\"):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Pass the dataset through the model\n",
    "    with torch.no_grad():\n",
    "        db_features = []\n",
    "        for batch in db_dataloader:\n",
    "            output = calc_compiled(batch)\n",
    "\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "average_time = sum(times) / len(times) # Calculate the average time\n",
    "\n",
    "print(f'Average time: {average_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [] # Initialize a list to store the time for each pass\n",
    "\n",
    "for _ in tqdm(range(ITERATIONS), desc=\"Processing query dataset\"):\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in q_dataloader:\n",
    "            output = calc_compiled(batch)\n",
    "\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "average_time = sum(times) / len(times) # Calculate the average time\n",
    "\n",
    "print(f'Average time: {average_time} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vprtutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
